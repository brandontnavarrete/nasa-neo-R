---
title: "nasa-neo-r"
author: "brandon navarrete"
date: "3/8/2023"
output: html_document
---
# Installs and library I will use
```{r}
install.packages('pacman')
library(pacman)

install.packages('lattice')
library(lattice)

install.packages('caret')
library(caret)

install.packages('tidyverse')
library(tidyverse)
```

# Let's pull data in. 

```{r}
df <- read_csv('neo_v2.csv')
```
# let's look at our data!
```{r}
head(df,5)
```

# set seed for consistency
```{r}
set.seed(42)
```

# Prepare

* id,name, orbiting_body,sentry_object where all dropped due to not being helpful, repeating information

* target variable one-hot encoded for m.l

* the outliers were left in

* nulls not present


```{r}
sum(is.na(df))
```
subset to drop columns
```{r}
df = subset(df,select = -c(id,name,orbiting_body, sentry_object))
```
```{r}
df
```

# Explore

* How many objects are inert?

* Will diameter have a role in hazard status?

* Will absolute magnitude have a role in hazard status?

* Will velocity have a role in hazard status?

# Data Splitting

```{r}
# converting boolean to numbers (0 = safe, 1 = hazardous)
df$hazardous <- as.numeric(df$hazardous)
```

```{r}
# creating a percentage split for training data vs testing data
training_index <- createDataPartition(df$hazardous, p=0.8, list = FALSE)
```

```{r}
# selecting data based on index
training_set <- df[training_index,] # train set
testing_set <- df[-training_index,] # train set

```

```{r}
# let's look at how much data we have per set vs the Whole dataframe.
nrow(training_set) 
nrow(testing_set)
nrow(df)
```
```{r}
# factor our y variable, of zero and one.
training_set$hazardous <- factor(training_set$hazardous)
testing_set$hazardous <- factor(testing_set$hazardous)
```

```{r}
# Load e1071 package
library(e1071)
```

# Model Creation
```{r}
# fit GLM model
model <- train(hazardous ~ ., data = training_set, method = "glm")

```

```{r}
summary(model)
```
# predict from model
```{r}
# Get predicted probabilities for the test set
predictions <- predict(model, newdata = testing_set, type = "prob")

```

```{r}
predictions
```

```{r}
# Set decision threshold to 0.05
threshold <- 0.05

# Create vector of predicted classes based on the threshold
predicted_classes <- ifelse(predictions[, "1"] > threshold, 1, 0)

```

```{r}
```{r}
# fit GLM model
model <- train(hazardous ~ ., data = training_set, method = "glm")

```

```{r}
summary(model)
```
# predict from model
```{r}
# Get predicted probabilities for the test set
predictions <- predict(model, newdata = testing_set, type = "prob")

```

```{r}
predictions
```

```{r}
# Set decision threshold to 0.05
threshold <- 0.05

# Create vector of predicted classes based on the threshold
predicted_classes <- ifelse(predictions[, "1"] > threshold, 1, 0)

```

```{r}
# Create confusion matrix for the predicted classes
confusion_matrix <- table(predicted_classes, testing_set$hazardous)
```

```{r}
print(confusion_matrix)
```
```{r}
# This is much better. At the expense of more false positves, i have now captured..ALMOST 99%!
```


```{r}
# Create confusion matrix plot

confusionMatrix <- confusionMatrix(confusion_matrix)
plot_confusion <- plot(confusionMatrix$table, col = confusionMatrix$byClass, 
                       main = paste("Accuracy = ", round(confusionMatrix$overall['Accuracy'], 3)), 
                       xlab = "Predicted Class", ylab = "True Class")
```


```{r}
confusionMatrix <- confusionMatrix(confusion_matrix)

plot_confusion <- plot(confusionMatrix$table, col = confusionMatrix$byClass, 
                       main = paste("Accuracy = ", round(confusionMatrix$overall['Accuracy'], 3)), 
                       xlab = "Predicted Class", ylab = "True Class") + 
                 geom_rect(xmin = 0.5, xmax = 1.5, ymin = 0.5, ymax = 1.5, fill = "lightgray", 
                           color = "black", alpha = 0, size = 2) + # highlight TN cells
                 geom_rect(xmin = 1.5, xmax = 2.5, ymin = 0.5, ymax = 1.5, fill = "darkgray", 
                           color = "black", alpha = 0, size = 2) # highlight FP cells
```







